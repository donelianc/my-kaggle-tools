{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "from os.path import abspath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add absolute path from root to sys.path to use custom modules\n",
    "sys.path.insert(0, abspath('..'))\n",
    "\n",
    "# Check if the path is already added to sys.path\n",
    "[p for p in sys.path if p.endswith(\"2022\")]\n",
    "\n",
    "# Import custom modules\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Author: Ayush Bihani\n",
    "# Source: https://www.kaggle.com/code/hsuyab/fast-loading-high-compression-with-feather/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "for dirname, _, filenames in os.walk(\"../data/raw\"):\n",
    "    for filename in filenames:\n",
    "        file_names.append(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths = []\n",
    "for k in file_names:\n",
    "    if 'train_' in k and 'dtypes' not in k:\n",
    "        train_paths.append(k)\n",
    "        \n",
    "train_paths.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(train_paths[0])\n",
    "# df_test = reduce_mem_usage(df_test)\n",
    "# \n",
    "# df_test.head()\n",
    "# df_test.game_num.nunique()\n",
    "# df_test.groupby([\"game_num\", \"event_id\"])[[\"event_time\"]].count() # .describe()\n",
    "# df_test[(df_test.game_num == 1) & (df_test.event_id == 1002)].event_time.abs().round(-1).value_counts().sort_index()\n",
    "# \n",
    "# df_sampled = df_test.groupby([\"game_num\", \"event_id\"], group_keys=False).apply(lambda x: x.sample(frac=0.1))\n",
    "# df_sampled.groupby([\"game_num\", \"event_id\"])[[\"event_time\"]].count()\n",
    "# df_sampled[(df_sampled.game_num == 1) & (df_sampled.event_id == 1002)].event_time.abs().round(-1).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory, ../data/interim/feather_data, has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 100.03 MB\n",
      "Memory usage after optimization is: 24.60 MB\n",
      "Decreased by 75.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:34<05:10, 34.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 100.94 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:07<04:30, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.82 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 98.92 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:40<03:53, 33.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.32 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 98.41 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:14<03:20, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.20 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 98.56 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:48<02:48, 33.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.24 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 97.44 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:21<02:13, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 23.96 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 96.06 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:53<01:38, 32.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 23.62 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 98.17 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:26<01:05, 32.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.14 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 100.12 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [04:59<00:32, 32.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.62 MB\n",
      "Decreased by 75.4%\n",
      "Memory usage of dataframe is 97.87 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:32<00:00, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 24.07 MB\n",
      "Decreased by 75.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_FRAC = 0.1\n",
    "if not makedir_check(\"../data/interim/feather_data\"):\n",
    "\n",
    "    time_read = {}\n",
    "    size_file = {}\n",
    "    \n",
    "    for file_path in tqdm(train_paths):\n",
    "        \n",
    "        # ext can be feather\n",
    "        times = {}\n",
    "        sizes = {}\n",
    "        \n",
    "        # stores name of file to be saved as \n",
    "        name_files_ftr =  (\n",
    "            file_path.split(\"/\")[-1]\n",
    "            .replace(\".\", \"_compressed.\")\n",
    "            .replace(\"csv\",\"ftr\")\n",
    "        )\n",
    "        \n",
    "        # store only file name without extension\n",
    "        file_only = name_files_ftr.split(\"_c\")[0]\n",
    "\n",
    "        #read file csv\n",
    "        t1 = time()\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if SAMPLE_FRAC < 1:\n",
    "            df = (\n",
    "                df\n",
    "                .groupby([\"game_num\", \"event_id\"], group_keys=False)\n",
    "                .apply(lambda x: x.sample(frac=SAMPLE_FRAC))\n",
    "                .reset_index(drop=True)\n",
    "                )\n",
    "\n",
    "        sizes[\"memory_csv\"] = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "        times[\"read_csv\"] = time() - t1\n",
    "        df = reduce_mem_usage(df)\n",
    "        \n",
    "        # save to feather\n",
    "        df.to_feather(\"../data/interim/feather_data/\"+name_files_ftr)\n",
    "        \n",
    "        # calculate reading time for feather\n",
    "        t1 = time()\n",
    "        df = pd.read_feather(\"../data/interim/feather_data/\"+name_files_ftr)\n",
    "        sizes[\"memory_feather\"] = df.memory_usage(deep=True).sum()/(1024**2)\n",
    "        times[\"read_feather\"] = time() - t1\n",
    "        \n",
    "        # store size and time for a particular file\n",
    "        time_read[file_only] = times\n",
    "        size_file[file_only] = sizes\n",
    "\n",
    "\n",
    "    time_taken_read = pd.DataFrame(time_read).T\n",
    "    # rounding of time to 3 decimal places\n",
    "    time_taken_read = time_taken_read.apply(lambda x: x.round(3))\n",
    "    time_taken_read.columns = ['time_to_read_csv(seconds)', 'time_to_read_feather(seconds)']\n",
    "\n",
    "\n",
    "    size_of_files = pd.DataFrame(size_file).T\n",
    "    size_of_files = size_of_files.apply(lambda x: x.round(2))\n",
    "    size_of_files.columns = ['memory_csv(MB)', 'memory_feather(MB)']\n",
    "    size_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_df = pd.read_csv(\"../data/raw/train_dtypes.csv\")\n",
    "dtypes = {k: v for (k, v) in zip(dtypes_df.column, dtypes_df.dtype)}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(len(train_paths)):\n",
    "    df_aux = pd.read_feather(f\"../data/interim/feather_data/train_{i}_compressed.ftr\")\n",
    "    df_aux = df_aux.astype(dtypes)\n",
    "    df = pd.concat([df, df_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 450.80 MB\n",
      "Memory usage after optimization is: 258.76 MB\n",
      "Decreased by 42.6%\n"
     ]
    }
   ],
   "source": [
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True).to_feather(f\"../data/interim/sample_{round(SAMPLE_FRAC*100)}perc_train_compressed.ftr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kaggle-main')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fd4831d96ed66bca442039d176b10bc5346e2d2477946333cb9e6bec5a9b6e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
